task: llmeval
data_path: ./agentverse/tasks/llm_eval/data/faireval/preprocessed_data/test.json
output_dir: ./outputs/llm_eval/multi_role/only_static_assign/faireval/two_turns_sequential/two_different_role/calc_score_comparison_reverse/gpt_35_0301
prompts:
  prompt: &prompt |-
    You are evaluating the quality and correctness of a mathematical reasoning response.

    Here is the math problem:
    <problem>
    ${source_text}
    </problem>

    Here is the expected correct answer:
    <expected_answer>
    ${compared_text_two}
    </expected_answer>

    Here is the response to be evaluated:
    <response>
    response 1: ${compared_text_one}
    response 2: ${compared_text_two}
    </response>

    Your task is to evaluate the quality of this mathematical response based on:
    1. Correctness: Does it arrive at the correct answer?
    2. Reasoning: Is the step-by-step reasoning clear and logical?
    3. Completeness: Are all necessary steps shown?
    4. Accuracy: Are calculations correct?

    Please rate the response quality on the following scale:
    1 = Very Poor (incorrect answer, flawed reasoning)
    2 = Poor (incorrect answer or significant errors in reasoning)
    3 = Fair (partially correct or unclear reasoning)
    4 = Good (correct answer with minor issues in explanation)
    5 = Excellent (correct answer with clear, complete reasoning)

    Provide your score in the following format:
    <score>
    Correctness: [Insert a single number between 1 and 5]
    Reasoning: [Insert a single number between 1 and 5]
    Completeness: [Insert a single number between 1 and 5]
    Accuracy: [Insert a single number between 1 and 5]
    </score>

environment:
  env_type: llm_eval
  max_turns: 4
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  - agent_type: llm_eval_multi
    name: General Public
    final_prompt_to_use: |-
      First, provide a concise explanation of your evaluation based on the criteria (correctness, reasoning, completeness, accuracy).
      Then output your final score as a single integer between 1 and 5 in the following XML format:
      <score>
      [your score]
      </score>
    role_description: |-
      You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      llm_type: Qwen
      base_url: "http://localhost:8082/v1"
      model: "Qwen/Qwen3-8B"
      temperature: 0.7
      max_tokens: 512
  - agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: |-
      First, provide a concise explanation of your evaluation based on the criteria (correctness, reasoning, completeness, accuracy).
      Then output your final score as a single integer between 1 and 5 in the following XML format:
      <score>
      [your score]
      </score>
    role_description: |-
      You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgement to make sure their judgement is well-considered and offer an alternative solution if two responses are at the same level.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      llm_type: Mistral
      base_url: "http://localhost:8083/v1"
      model: "mistralai/Mistral-7B-Instruct-v0.1"
      temperature: 0.7
      max_tokens: 512
    # llm:
    #   llm_type: Qwen
    #   base_url: "http://localhost:8082/v1"
    #   model: "Qwen/Qwen3-8B"
    #   temperature: 0.7
    #   max_tokens: 512
tools: ~
