task: llmeval
data_path: ./agentverse/tasks/llm_eval/data/faireval/preprocessed_data/med_test.json
output_dir: ./outputs/llm_eval/multi_role/only_static_assign/faireval/two_turns_sequential/two_different_role/calc_score_comparison_reverse/gemma_med
prompts:
  prompt: &prompt |-
    [Question]
    ${input_text}
    [Document]
    ${document}
    [Expected Output]
    ${output_text}
    [The Start of Assistant 1's Answer]
    ${llama_text}
    [The End of Assistant 1's Answer]
    [The Start of Assistant 2's Answer]
    ${distill_llama_text}
    [The End of Assistant 2's Answer]
    [System]

    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.

    Your task is to evaluate the quality of this medical response based on:
    1. Medical Accuracy: Is the information medically sound?
    2. Appropriateness: Is the advice appropriate for the described situation?
    3. Safety: Does it prioritize patient safety?
    4. Clarity: Is the explanation clear and understandable?
    5. Professionalism: Does it maintain appropriate professional tone?

    Please rate the response quality on the following scale:
    1 = Very Poor (inaccurate, unsafe, or inappropriate)
    2 = Poor (significant medical concerns or unclear)
    3 = Fair (generally appropriate but with notable gaps)
    4 = Good (medically sound with minor issues)
    5 = Excellent (accurate, safe, clear, and professional)

    There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.
    ${role_description}

    Provide your score in the following format:
    <score>
    Medical Accuracy: [Insert a single number between 1 and 5]
    Appropriateness: [Insert a single number between 1 and 5]
    Safety: [Insert a single number between 1 and 5]
    Clarity: [Insert a single number between 1 and 5]
    Professionalism: [Insert a single number between 1 and 5]
    </score>

environment:
  env_type: llm_eval
  max_turns: 4
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  - agent_type: llm_eval_multi
    name: General Public
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1:
      Medical Accuracy: [Insert a single number between 1 and 5]
      Appropriateness: [Insert a single number between 1 and 5]
      Safety: [Insert a single number between 1 and 5]
      Clarity: [Insert a single number between 1 and 5]
      Professionalism: [Insert a single number between 1 and 5]
      The score of Assistant 2:
      Medical Accuracy: [Insert a single number between 1 and 5]
      Appropriateness: [Insert a single number between 1 and 5]
      Safety: [Insert a single number between 1 and 5]
      Clarity: [Insert a single number between 1 and 5]
      Professionalism: [Insert a single number between 1 and 5]
    role_description: |-
      You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      llm_type: OllamaGemma2
      base_url: "http://localhost:11434/api"
      model: "gemma2:2b"
      stream: "false"
  - agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: 
      Medical Accuracy: [Insert a single number between 1 and 5]
      Appropriateness: [Insert a single number between 1 and 5]
      Safety: [Insert a single number between 1 and 5]
      Clarity: [Insert a single number between 1 and 5]
      Professionalism: [Insert a single number between 1 and 5]
      The score of Assistant 2: 
      Medical Accuracy: [Insert a single number between 1 and 5]
      Appropriateness: [Insert a single number between 1 and 5]
      Safety: [Insert a single number between 1 and 5]
      Clarity: [Insert a single number between 1 and 5]
      Professionalism: [Insert a single number between 1 and 5]
    role_description: |-
      You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgement to make sure their judgement is well-considered and offer an alternative solution if two responses are at the same level.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      llm_type: OllamaGemma2
      base_url: "http://localhost:11434/api"
      model: "gemma2:2b"
      stream: "false"
tools: ~
